---
title: "Trabajo Práctico 3 - Modelo de Lenguaje N-gramas"
author: "Luca Mazzarello, Ignacio Pardo y Juan Ignacio Silvestri"
date: "Noviembre 2024"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

# Introducción

Este trabajo implementa un modelo de lenguaje basado en n-gramas, entrenado con scripts de la serie Friends. El objetivo es generar texto nuevo que mantenga características del corpus original, analizando la calidad y coherencia de los resultados obtenidos con diferentes configuraciones del modelo.

# Metodología

## Corpus y Preprocesamiento

El corpus utilizado consiste en scripts de la serie Friends, específicamente diálogos de múltiples temporadas. Para el procesamiento del corpus, implementamos las siguientes etapas:

### Carga del Corpus

El corpus se carga desde archivos de texto que contienen los scripts de Friends:

```python
def corpus_from_file(file_path: str) -> Corpus:
    with open(file_path, "r") as file:
        return file.readlines()

corpus = corpus_from_file("friends_corpus.txt")
```

### Preprocesamiento y Tokenización

El modelo implementa un preprocesamiento específico para scripts, que incluye:

1. **Tokens Especiales**:
   - `<e>`, `</e>`: Marcadores de inicio y fin de episodio
   - `<s>`, `</s>`: Marcadores de inicio y fin de escena
   - `<l>`, `</l>`: Marcadores de líneas de diálogo

2. **Estructura del Script**:
   - Detección automática de escenas (`[Scene...]`)
   - Identificación de finales de episodio (`End`)
   - Manejo de líneas de diálogo

```python
def fit(self, corpus: Corpus, verbose=False, type="text"):
    """
    Procesa el corpus y extrae n-gramas manteniendo la estructura del script
    """
    all_n_grams = []
    all_tokens = []

    # Iteramos sobre el corpus
    scene_started = False
    episode_started = False
    
    for text in corpus:
        if text == "\n":
            continue
            
        # Tokenizamos el texto
        tokens = self.tokenize(text)
        
        # Manejamos estructura del script
        if not episode_started:
            tokens = [EPISODE_START] + tokens
            episode_started = True
            
        if ("[scene" in text) or ("[Scene" in text):
            if scene_started:
                tokens = tokens + [SCENE_END]
            tokens = [SCENE_START] + tokens
            scene_started = True
            
        if "\nEnd\n" in text:
            tokens = tokens + [SCENE_END, EPISODE_END]
            scene_started = False
            episode_started = False

        # Extraemos n-gramas
        n_grams = self.extract_ngrams(tokens)
        all_n_grams.append(n_grams)
        all_tokens.append(tokens)
```

Este preprocesamiento nos permite:
- Mantener la estructura jerárquica del script (episodios → escenas → diálogos)
- Generar texto que respeta el formato de guión
- Capturar patrones de inicio y fin de escenas/episodios
- Preservar el contexto conversacional entre personajes

Los tokens especiales ayudan al modelo a aprender:
- Transiciones naturales entre escenas
- Estructura de los diálogos
- Patrones de interacción entre personajes

## Implementación del Modelo N-gramas

El modelo implementa tres funcionalidades principales:

### 1. Construcción del Modelo

El proceso de construcción del modelo incluye:

```python
def extract_ngrams(self, tokens, verbose=False):
    """Extrae todos los n-gramas del texto tokenizado"""
    n_grams = []
    for i in range(len(tokens) - self.n + 1):
        n_grams.append(tuple(tokens[i : i + self.n]))
    return n_grams

def create_frequency_table(self, n_grams, verbose=False):
    """Crea tabla de frecuencias a partir de los n-gramas"""
    # Aplanamos la lista de n-gramas
    n_grams = [item for sublist in n_grams for item in sublist]
    
    # Creamos vocabulario y contamos frecuencias
    self.vocav = set(n_grams)
    n_gram_counts = Counter(n_grams)
    
    # Construimos tabla de frecuencias
    frequency_table = {}
    for n_gram in n_grams:
        if n_gram[:-1] not in frequency_table:
            frequency_table[n_gram[:-1]] = {}
        if n_gram[-1] not in frequency_table[n_gram[:-1]]:
            frequency_table[n_gram[:-1]][n_gram[-1]] = 0
        frequency_table[n_gram[:-1]][n_gram[-1]] += 1
```

### 2. Predicción y Generación

El modelo puede generar texto a diferentes niveles:

#### a) Generación de líneas individuales
```python
def generate(self, context: Tuple[str] = "", temperature: float = 0.9) -> str:
    """Genera una línea de texto dado un contexto inicial"""
    max_length = 30
    if context == "":
        context = [SENTENCE_START] * (self.n - 1)
    else:
        context = self.tokenize(context)[:-1]
        
    while next_word != SENTENCE_END and max_length > 0:
        # Obtener distribución de probabilidad
        logits = self.predict(context)
        if logits is None:
            break
            
        # Aplicar temperatura para controlar aleatoriedad
        logits = {k: v ** (1 / temperature) for k, v in logits.items() if v > 0}
        logits = {k: v / sum(logits.values()) for k, v in logits.items()}
        
        # Seleccionar siguiente palabra
        next_word = np.random.choice(list(logits.keys()), p=list(logits.values()))
        answer += f" {next_word}"
        context = context[1:] + [next_word]
        max_length -= 1
```

#### b) Generación de escenas
```python
def generate_scene(self, context: str = "", temperature: float = 0.9, max_sentences=10) -> str:
    """Genera una escena completa manteniendo coherencia entre líneas"""
    scene = context
    while SCENE_END not in scene and max_sentences > 0:
        sentence = self.generate(context, temperature)
        if SCENE_START in sentence:
            break
        scene += sentence
        context = sentence[-2:]
        max_sentences -= 1
```

#### c) Generación de episodios completos
```python
def generate_episode(self, context: str = "", temperature: float = 0.9, 
                    max_scenes=10, max_lines_per_scene=10) -> str:
    """Genera un episodio completo con múltiples escenas"""
    episode = context
    while EPISODE_END not in episode and max_scenes > 0:
        sentence = self.generate_scene(context, temperature, max_lines_per_scene)
        if EPISODE_START in sentence:
            break
        episode += sentence
        context = sentence[-2:]
        max_scenes -= 1
```

### 3. Control de Generación

El modelo incluye varios parámetros para controlar la generación:

- **Temperature**: Controla la aleatoriedad de la generación
  - T < 1: Más determinístico, selecciona palabras más probables
  - T > 1: Más aleatorio, aumenta la probabilidad de palabras menos comunes

- **Límites de longitud**:
  - `max_length`: Longitud máxima de líneas individuales
  - `max_sentences`: Número máximo de líneas por escena
  - `max_scenes`: Número máximo de escenas por episodio

### Ejemplo de Uso

```python
# Inicializar y entrenar modelo
model = Ngram(n=3)  # Trigrama
model.fit(train_corpus)

# Generar una línea de diálogo
new_line = model.generate(context="Joey: Hey", temperature=1)

# Generar una escena
new_scene = model.generate_scene(
    context="[Scene: Central Perk] \n", 
    temperature=0.5, 
    max_sentences=100
)

# Generar un episodio completo
title = "The One with the Gun"
episode = model.generate_episode(
    context=title, 
    temperature=0.4, 
    max_lines_per_scene=100
)
```

# Resultados y Análisis

## Calidad de Textos Generados

Analizamos cómo varía la calidad de los textos generados en función de:

### Valor de n
- n=1: Palabras aleatorias sin contexto
- n=2: Pares coherentes pero sin estructura global
- n=3-4: Mejor balance entre coherencia local y estructura
- n>4: Sobreajuste, repite fragmentos exactos

### Cantidad de datos de entrenamiento
- Impacto en el vocabulario
- Mejora en la fluidez
- Límites de generalización

### Ejemplos de Textos Generados

```python
for n in [2,3,4]:
    print(f"n={n}:")
    print(model.generate())
```

## Similitud con Textos Originales


## Análisis del Grado de Creatividad e Inteligencia

### Grado de Creatividad

El modelo exhibe un grado de creatividad fundamentalmente limitado y superficial, que podemos analizar en tres niveles diferentes:

1. **Creatividad Combinatoria**
Es capaz de generar nuevas combinaciones de frases y expresiones que no existen exactamente en el corpus original. Por ejemplo, puede crear diálogos entre personajes que nunca ocurrieron en la serie, como interacciones entre Joey y Phoebe que mantienen sus personalidades características pero en situaciones nuevas. Sin embargo, estas combinaciones son puramente estadísticas, basadas en la frecuencia de aparición de patrones en el corpus de entrenamiento. En términos de creatividad combinatoria, el modelo alcanza un grado medio de desempeño. 

2. **Creatividad Estructural**
Si bien puede reproducir la estructura básica de un guión de Friends, con sus marcadores de escena y diálogos, no es capaz de crear nuevas estructuras narrativas. El modelo está confinado a los patrones estructurales presentes en el corpus de entrenamiento, sin poder innovar en la forma de contar historias o presentar diálogos. Esto se evidencia en cómo tiende a caer en patrones repetitivos y estructuras conversacionales predecibles. La creatividad estructural del modelo es baja.

3. **Creatividad Conceptual**
No puede generar ideas o conceptos verdaderamente nuevos, ni crear tramas originales coherentes. Aunque puede reproducir el estilo humorístico de Friends, no comprende realmente el humor que está generando. Es particularmente notable cómo el modelo puede reproducir chistes o situaciones cómicas similares a las del corpus, pero no puede crear humor original que requiera comprensión contextual o timing. En cuanto a la creatividad conceptual, el modelo muestra un grado muy bajo.

### Grado de Inteligencia

La "inteligencia" del modelo se manifiesta de manera desigual en diferentes aspectos:

1. **Inteligencia Lingüística**
Demuestra competencia en el manejo de la estructura básica del lenguaje y es particularmente efectivo en reproducir el estilo conversacional característico de Friends. Sin embargo, esta competencia es superficial: el modelo no comprende realmente el significado de lo que genera, como se evidencia en las frecuentes contradicciones semánticas y la falta de coherencia en conversaciones extensas. En este ámbito podriamos decir que el modelo alcanza un grado medio-bajo. 

2. **Inteligencia Narrativa**
No puede mantener coherencia en historias largas ni establecer relaciones causales lógicas entre eventos. Un ejemplo claro es cómo el modelo puede comenzar una escena con una premisa específica pero rápidamente pierde el hilo narrativo, derivando en diálogos inconexos o contradictorios. La incapacidad de planificar el desarrollo de una trama demuestra las limitaciones fundamentales del enfoque basado en n-gramas. Para este caso el modelo tiene un grado muy bajo.

3. **Inteligencia Social**
Puede reproducir interacciones sociales básicas y mantener ciertos aspectos de las personalidades de los personajes, como el sarcasmo de Chandler o la ingenuidad de Joey. Sin embargo, no comprende realmente las relaciones entre los personajes ni puede generar interacciones que reflejen un entendimiento profundo de sus dinámicas sociales. Para este caso el modelo tiene un grado bajo.

### Conclusión sobre Creatividad e Inteligencia

El análisis de este modelo n-grama revela las limitaciones fundamentales de los enfoques puramente estadísticos para generar lenguaje. Si bien puede producir texto que superficialmente parece creativo e inteligente, un análisis más profundo revela que carece de las características fundamentales que definen la verdadera creatividad e inteligencia humana.

La aparente creatividad del modelo es en realidad un producto de la recombinación estadística de patrones existentes. No puede generar ideas verdaderamente nuevas ni comprender el significado más profundo de lo que produce. De manera similar, su "inteligencia" se limita a reconocer y reproducir patrones, sin verdadera comprensión o capacidad de razonamiento.

Estas limitaciones subrayan la diferencia fundamental entre la capacidad de procesar y recombinar información existente, y la verdadera creatividad e inteligencia que implican comprensión, intencionalidad y capacidad de innovación. El modelo n-grama, aunque impresionante en su capacidad de imitar ciertos aspectos del lenguaje humano, permanece fundamentalmente como una herramienta estadística sofisticada, lejos de alcanzar niveles de creatividad e inteligencia comparables a los humanos.
